{
  "providers": {
    "openai": {
      "api_key_env": "OPENAI_API_KEY",
      "base_url": "https://api.openai.com/v1",
      "default_model": "gpt-3.5-turbo",
      "max_tokens": 100,
      "temperature": 0.1
    },
    "anthropic": {
      "api_key_env": "ANTHROPIC_API_KEY", 
      "base_url": "https://api.anthropic.com/v1",
      "default_model": "claude-3-haiku-20240307",
      "max_tokens": 100,
      "temperature": 0.1
    },
    "ollama": {
      "api_key_env": null,
      "base_url": "http://localhost:11434",
      "default_model": "llama2",
      "max_tokens": 100,
      "temperature": 0.1
    },
    "custom": {
      "api_key_env": "CUSTOM_API_KEY",
      "base_url_env": "CUSTOM_LLM_URL",
      "default_model": "custom-model",
      "max_tokens": 100,
      "temperature": 0.1
    }
  },
  "usage_examples": {
    "openai": "export OPENAI_API_KEY='your-key-here' && python run_pipeline.py --llm-coref --llm-provider openai",
    "anthropic": "export ANTHROPIC_API_KEY='your-key-here' && python run_pipeline.py --llm-coref --llm-provider anthropic",
    "ollama": "ollama serve && python run_pipeline.py --llm-coref --llm-provider ollama --llm-model llama2",
    "custom": "export CUSTOM_LLM_URL='http://your-api.com/chat' && python run_pipeline.py --llm-coref --llm-provider custom"
  }
}
